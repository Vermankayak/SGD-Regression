{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boston House Prices dataset\n",
    "===========================\n",
    "\n",
    "Notes\n",
    "------\n",
    "Data Set Characteristics:  \n",
    "\n",
    "    :Number of Instances: 506 \n",
    "\n",
    "    :Number of Attributes: 13 numeric/categorical predictive\n",
    "    \n",
    "    :Median Value (attribute 14) is usually the target\n",
    "\n",
    "    :Attribute Information (in order):\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per $10,000\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in $1000's\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
    "\n",
    "This is a copy of UCI ML housing dataset.\n",
    "http://archive.ics.uci.edu/ml/datasets/Housing\n",
    "\n",
    "\n",
    "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
    "\n",
    "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
    "prices and the demand for clean air', J. Environ. Economics & Management,\n",
    "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
    "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
    "pages 244-261 of the latter.\n",
    "\n",
    "The Boston house-price data has been used in many machine learning papers that address regression\n",
    "problems.   \n",
    "     \n",
    "**References**\n",
    "\n",
    "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
    "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
    "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings #Code to remove warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import prettytable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston() #loading boston housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset is: (506, 13)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataset is:',boston.data.shape)#boston.data gives us an array. Here we are getting shape of boston array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos = pd.DataFrame(boston.data,columns = boston.feature_names)#Here we are converting our array 'boston.data' into a dataframe-\n",
    "#- and our column names are contained in 'boston.feature_names'\n",
    "bos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos['Price'] = boston.target #boston.target contains our y_i's and hence we are now putting our y_i's into bos dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bos.drop('Price',axis = 1)#Here we are splitting our dataset 'bos' into datapoint dataframe and y_i dataframe.\n",
    "y = bos['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y)#splitting our dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_train = StandardScaler().fit_transform(x_train) #standardizing our training dataset\n",
    "standardized_test = StandardScaler().fit_transform(x_test) #standardizing our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training dataframe which contains both datapoints and y_i's\n",
    "d_train = pd.DataFrame(standardized_train)\n",
    "d_train['Price'] = np.array(y_train)\n",
    "\n",
    "#Creating test dataframe which contains both datapoints and y_i's\n",
    "d_test = pd.DataFrame(standardized_test)\n",
    "d_test['Price'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 14)\n",
      "(127, 14)\n"
     ]
    }
   ],
   "source": [
    "print(d_train.shape)\n",
    "print(d_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.694276</td>\n",
       "      <td>-0.480911</td>\n",
       "      <td>0.998510</td>\n",
       "      <td>-0.248243</td>\n",
       "      <td>0.245557</td>\n",
       "      <td>-1.100563</td>\n",
       "      <td>0.071415</td>\n",
       "      <td>-0.828240</td>\n",
       "      <td>1.648985</td>\n",
       "      <td>1.537645</td>\n",
       "      <td>0.822072</td>\n",
       "      <td>-3.988520</td>\n",
       "      <td>0.637485</td>\n",
       "      <td>11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.390557</td>\n",
       "      <td>-0.480911</td>\n",
       "      <td>-0.186256</td>\n",
       "      <td>-0.248243</td>\n",
       "      <td>-0.073535</td>\n",
       "      <td>-0.321752</td>\n",
       "      <td>0.956263</td>\n",
       "      <td>-0.592255</td>\n",
       "      <td>-0.418155</td>\n",
       "      <td>0.143977</td>\n",
       "      <td>-0.300854</td>\n",
       "      <td>0.423459</td>\n",
       "      <td>0.627478</td>\n",
       "      <td>18.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.048540</td>\n",
       "      <td>-0.480911</td>\n",
       "      <td>1.215253</td>\n",
       "      <td>-0.248243</td>\n",
       "      <td>0.426662</td>\n",
       "      <td>-0.308451</td>\n",
       "      <td>0.870633</td>\n",
       "      <td>-0.721134</td>\n",
       "      <td>-0.532996</td>\n",
       "      <td>-0.028742</td>\n",
       "      <td>-1.751300</td>\n",
       "      <td>-1.335050</td>\n",
       "      <td>-0.413303</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.414197</td>\n",
       "      <td>0.981168</td>\n",
       "      <td>-0.760333</td>\n",
       "      <td>-0.248243</td>\n",
       "      <td>-1.056680</td>\n",
       "      <td>0.414203</td>\n",
       "      <td>-1.006101</td>\n",
       "      <td>0.841536</td>\n",
       "      <td>-0.303314</td>\n",
       "      <td>-0.469475</td>\n",
       "      <td>-1.096260</td>\n",
       "      <td>0.410557</td>\n",
       "      <td>-0.457622</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.401406</td>\n",
       "      <td>-0.480911</td>\n",
       "      <td>-0.101316</td>\n",
       "      <td>-0.248243</td>\n",
       "      <td>-0.573731</td>\n",
       "      <td>-0.618793</td>\n",
       "      <td>-1.651898</td>\n",
       "      <td>0.088378</td>\n",
       "      <td>-0.647837</td>\n",
       "      <td>-0.779178</td>\n",
       "      <td>0.073454</td>\n",
       "      <td>0.423459</td>\n",
       "      <td>-0.261760</td>\n",
       "      <td>22.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.694276 -0.480911  0.998510 -0.248243  0.245557 -1.100563  0.071415   \n",
       "1 -0.390557 -0.480911 -0.186256 -0.248243 -0.073535 -0.321752  0.956263   \n",
       "2 -0.048540 -0.480911  1.215253 -0.248243  0.426662 -0.308451  0.870633   \n",
       "3 -0.414197  0.981168 -0.760333 -0.248243 -1.056680  0.414203 -1.006101   \n",
       "4 -0.401406 -0.480911 -0.101316 -0.248243 -0.573731 -0.618793 -1.651898   \n",
       "\n",
       "          7         8         9        10        11        12  Price  \n",
       "0 -0.828240  1.648985  1.537645  0.822072 -3.988520  0.637485   11.7  \n",
       "1 -0.592255 -0.418155  0.143977 -0.300854  0.423459  0.627478   18.7  \n",
       "2 -0.721134 -0.532996 -0.028742 -1.751300 -1.335050 -0.413303   25.0  \n",
       "3  0.841536 -0.303314 -0.469475 -1.096260  0.410557 -0.457622   22.0  \n",
       "4  0.088378 -0.647837 -0.779178  0.073454  0.423459 -0.261760   22.6  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Custom implementation of SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgdreg_fit(x,alpha,batch_size=100, n_iter = 1000,eta0 = 0.01):\n",
    "    \n",
    "    #Here our learning rate == 'invscaling' and we have used L2 regularization \n",
    "    \"\"\"This function is custom implementation of Stochastic Gradient Descent Regressor.\n",
    "    \n",
    "    Input: \n",
    "          \n",
    "          x: This is our dataset on which we want to calculate our weights so that we can find a line which best fits-\n",
    "              our x.\n",
    "              \n",
    "          alpha: This is our coefficient of L2 Regularizer.\n",
    "          \n",
    "          batch_size: This is the size of our batch of our datapoints which we randomly sample from our dataset 'x' at each-\n",
    "                      -iteration.\n",
    "                      \n",
    "          n_iter: No. of iterations we want to perform.\n",
    "          \n",
    "          eta0: This is our learning rate.\n",
    "        \n",
    "    Output:\n",
    "           \n",
    "           This function return a dictionary containing weight and intercept corresponding to a particular alpha  value\"\"\"\n",
    "    \n",
    "    dict_1 = dict()#dictionary this function returns\n",
    "    t = 1 #invscaling parameter and is given by t = n_iter*batch_size\n",
    "    power_t = 0.25 #invscaling parameter which will be used as power of 't'\n",
    "    eta = eta0 #learning rate\n",
    "    \n",
    "    w_old = np.zeros(shape = (1,13))#zero intitialization of our weights.\n",
    "    b_old = 0 #zero initialization of our intercept term\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        gradient_w = np.zeros(shape = (1,13)) #zero intitialization of gradient of loss term w.r.t w\n",
    "        gradient_b = 0 #zero intitialization of gradient of loss term w.r.t b\n",
    "        batch_data=x.sample(n = batch_size) #randomly sampling datapoints from dataset\n",
    "        batch_x= np.array(batch_data.drop('Price',axis=1)) # separating our x_i's from batch dataset\n",
    "        batch_y = np.array(batch_data['Price']) # separating our y_i's from dataset from batch dataset\n",
    "        \n",
    "        #Here we are calculating gradient of loss term w.r.t w and w.r.t b\n",
    "        for j in range(batch_size):\n",
    "            y = np.dot(w_old,batch_x[j])+b_old \n",
    "            gradient_w += (batch_x[j]*(batch_y[j]-y)) + 2*alpha*w_old\n",
    "            gradient_b += (batch_y[j]-y)\n",
    "       \n",
    "        gradient_w *= (-2/batch_size)\n",
    "        gradient_b *= (-2/batch_size)\n",
    "        \n",
    "        #updating our weights\n",
    "        w_new = w_old - eta*gradient_w\n",
    "        b_new = b_old - eta*gradient_b\n",
    "        \n",
    "        #updating our old weights\n",
    "        w_old = w_new\n",
    "        b_old = b_new\n",
    "        eta = eta/pow(t,power_t) #Here we are updating our learning rate at each iteration\n",
    "        t=(i*batch_size)+(alpha*eta0) #updating our invscaling parameter at each iteration\n",
    "        \n",
    "        #Here we are assigning values to various keys in the dictionary\n",
    "        if i == (n_iter-1)  : \n",
    "            dict_1['alpha'] = alpha\n",
    "            dict_1['weight'] = w_old\n",
    "            dict_1['intercept'] = b_old\n",
    "            break\n",
    "    return dict_1\n",
    "\n",
    "\n",
    "def sgdreg_predict(w,b,x):\n",
    "    \"\"\"This function will predict the y_i values corresponding to our query points.\n",
    "        \n",
    "       Input: This function takes 'w' which is the best weight on which our dataset has been trained. \n",
    "              This function also takes 'b' which is the best intercept term on which our dataset has been trained.\n",
    "              'x' is our test dataset.\n",
    "        \n",
    "       Output: This function returns an array of predicted values.\n",
    "       \n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    for i in range(x.shape[0]):\n",
    "        y_pred.append(np.dot(w,x[i])+b)\n",
    "    return np.array(y_pred)\n",
    "\n",
    "def error(y_true,y_pred):\n",
    "    \"\"\"This function calculates mean squared error\"\"\"\n",
    "    error = mean_squared_error(y_true,y_pred)\n",
    "    return error\n",
    "\n",
    "def plot(y_true,y_pred):\n",
    "    \"\"\"This function plots y_pred as a function of y_true\"\"\"\n",
    "    plt.figure()\n",
    "    plt.scatter(y = y_pred,x = y_true,label = 'Actual values Vs Pred values')\n",
    "    plt.xlabel('Actual values')\n",
    "    plt.ylabel('Predicted values')\n",
    "    plt.title('Actual values Vs Pred values')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 1e-06, eta = 1e-06, mse = 585.5912974903646\n",
      "alpha = 1e-06, eta = 1e-05, mse = 568.4120011329558\n",
      "alpha = 1e-06, eta = 0.0001, mse = 477.42740964084373\n",
      "alpha = 1e-06, eta = 0.001, mse = 185.4550531700009\n",
      "alpha = 1e-06, eta = 0.01, mse = 537.4241268396811\n",
      "alpha = 1e-06, eta = 0.1, mse = 9102493.203844715\n",
      "alpha = 1e-06, eta = 1, mse = 8.009229853387747e+17\n",
      "alpha = 1e-06, eta = 10, mse = 1.4330462550748914e+31\n",
      "alpha = 1e-06, eta = 100, mse = 2.418153055558553e+44\n",
      "alpha = 1e-06, eta = 1000, mse = 2.202044048965857e+60\n",
      "alpha = 1e-06, eta = 10000, mse = 7.517358960021271e+76\n",
      "alpha = 1e-06, eta = 100000, mse = 5.42896297609114e+95\n",
      "alpha = 1e-06, eta = 1000000, mse = 6.164461811029461e+115\n",
      "alpha = 1e-05, eta = 1e-06, mse = 588.0098637370008\n",
      "alpha = 1e-05, eta = 1e-05, mse = 577.1803377850713\n",
      "alpha = 1e-05, eta = 0.0001, mse = 522.5942352294663\n",
      "alpha = 1e-05, eta = 0.001, mse = 304.980486739748\n",
      "alpha = 1e-05, eta = 0.01, mse = 115.92005296807415\n",
      "alpha = 1e-05, eta = 0.1, mse = 112224.76569128312\n",
      "alpha = 1e-05, eta = 1, mse = 250854540109324.4\n",
      "alpha = 1e-05, eta = 10, mse = 3.6313072481568044e+27\n",
      "alpha = 1e-05, eta = 100, mse = 5.216930398743069e+40\n",
      "alpha = 1e-05, eta = 1000, mse = 2.0315855043969676e+56\n",
      "alpha = 1e-05, eta = 10000, mse = 2.533443197260906e+71\n",
      "alpha = 1e-05, eta = 100000, mse = 2.279086692778207e+90\n",
      "alpha = 1e-05, eta = 1000000, mse = 2.5926389448093135e+109\n",
      "alpha = 0.0001, eta = 1e-06, mse = 588.6556485071422\n",
      "alpha = 0.0001, eta = 1e-05, mse = 583.939844015577\n",
      "alpha = 0.0001, eta = 0.0001, mse = 546.6686214787708\n",
      "alpha = 0.0001, eta = 0.001, mse = 396.19842941480357\n",
      "alpha = 0.0001, eta = 0.01, mse = 118.58217493958155\n",
      "alpha = 0.0001, eta = 0.1, mse = 129.49675125485834\n",
      "alpha = 0.0001, eta = 1, mse = 193030361562845.47\n",
      "alpha = 0.0001, eta = 10, mse = 2.011303982031902e+25\n",
      "alpha = 0.0001, eta = 100, mse = 8.476733738085556e+37\n",
      "alpha = 0.0001, eta = 1000, mse = 3.350330688159265e+52\n",
      "alpha = 0.0001, eta = 10000, mse = 1.561918008825844e+67\n",
      "alpha = 0.0001, eta = 100000, mse = 1.0852399927807173e+85\n",
      "alpha = 0.0001, eta = 1000000, mse = 1.859848793668928e+101\n",
      "alpha = 0.001, eta = 1e-06, mse = 589.1304382682057\n",
      "alpha = 0.001, eta = 1e-05, mse = 585.1589316111069\n",
      "alpha = 0.001, eta = 0.0001, mse = 567.3117392074686\n",
      "alpha = 0.001, eta = 0.001, mse = 475.0369597557927\n",
      "alpha = 0.001, eta = 0.01, mse = 167.235657987466\n",
      "alpha = 0.001, eta = 0.1, mse = 133.9215100752884\n",
      "alpha = 0.001, eta = 1, mse = 3868954367481.011\n",
      "alpha = 0.001, eta = 10, mse = 6.3558837525549825e+22\n",
      "alpha = 0.001, eta = 100, mse = 2.0570200182063595e+35\n",
      "alpha = 0.001, eta = 1000, mse = 3.320848755307668e+48\n",
      "alpha = 0.001, eta = 10000, mse = 7.882480513923244e+63\n",
      "alpha = 0.001, eta = 100000, mse = 1.990340932481087e+78\n",
      "alpha = 0.001, eta = 1000000, mse = 1.1019556473273775e+85\n",
      "alpha = 0.01, eta = 1e-06, mse = 589.5331418922383\n",
      "alpha = 0.01, eta = 1e-05, mse = 587.5047153322602\n",
      "alpha = 0.01, eta = 0.0001, mse = 577.7721804445258\n",
      "alpha = 0.01, eta = 0.001, mse = 513.2410160221331\n",
      "alpha = 0.01, eta = 0.01, mse = 286.70408439715857\n",
      "alpha = 0.01, eta = 0.1, mse = 76.43598821814052\n",
      "alpha = 0.01, eta = 1, mse = 51200544325.59219\n",
      "alpha = 0.01, eta = 10, mse = 7.415099013195343e+19\n",
      "alpha = 0.01, eta = 100, mse = 1.1663430012946158e+31\n",
      "alpha = 0.01, eta = 1000, mse = 9.662383159929995e+44\n",
      "alpha = 0.01, eta = 10000, mse = 2.0838921599906294e+57\n",
      "alpha = 0.01, eta = 100000, mse = 9.507716672444089e+65\n",
      "alpha = 0.01, eta = 1000000, mse = 1.2042550171710226e+71\n",
      "alpha = 0.1, eta = 1e-06, mse = 589.7197923197265\n",
      "alpha = 0.1, eta = 1e-05, mse = 588.7766705999273\n",
      "alpha = 0.1, eta = 0.0001, mse = 582.039363647325\n",
      "alpha = 0.1, eta = 0.001, mse = 542.778295460211\n",
      "alpha = 0.1, eta = 0.01, mse = 375.895396002623\n",
      "alpha = 0.1, eta = 0.1, mse = 37.84139064712439\n",
      "alpha = 0.1, eta = 1, mse = 2298949985.32893\n",
      "alpha = 0.1, eta = 10, mse = 3.121476785280179e+17\n",
      "alpha = 0.1, eta = 100, mse = 1.5247348689014173e+29\n",
      "alpha = 0.1, eta = 1000, mse = 2.341964632304869e+40\n",
      "alpha = 0.1, eta = 10000, mse = 4.357746710036582e+49\n",
      "alpha = 0.1, eta = 100000, mse = 2.463797563850044e+53\n",
      "alpha = 0.1, eta = 1000000, mse = 1.2181500083002381e+62\n",
      "alpha = 1, eta = 1e-06, mse = 589.7914628342031\n",
      "alpha = 1, eta = 1e-05, mse = 589.2471555581494\n",
      "alpha = 1, eta = 0.0001, mse = 584.9499016077467\n",
      "alpha = 1, eta = 0.001, mse = 561.1279870210752\n",
      "alpha = 1, eta = 0.01, mse = 428.8074873112043\n",
      "alpha = 1, eta = 0.1, mse = 126.25766847743176\n",
      "alpha = 1, eta = 1, mse = 658471.0725046844\n",
      "alpha = 1, eta = 10, mse = 212259149021304.56\n",
      "alpha = 1, eta = 100, mse = 1.7627702767042736e+21\n",
      "alpha = 1, eta = 1000, mse = 1.377141307715177e+32\n",
      "alpha = 1, eta = 10000, mse = 3.2288589275226414e+39\n",
      "alpha = 1, eta = 100000, mse = 3.2795591482934436e+46\n",
      "alpha = 1, eta = 1000000, mse = 5.0989011172160714e+54\n",
      "alpha = 10, eta = 1e-06, mse = 589.8684540068017\n",
      "alpha = 10, eta = 1e-05, mse = 589.4623167200921\n",
      "alpha = 10, eta = 0.0001, mse = 586.8519001738928\n",
      "alpha = 10, eta = 0.001, mse = 569.4331812978122\n",
      "alpha = 10, eta = 0.01, mse = 460.91006264374005\n",
      "alpha = 10, eta = 0.1, mse = 213508.874385556\n",
      "alpha = 10, eta = 1, mse = 881608472975.4432\n",
      "alpha = 10, eta = 10, mse = 1.0360042164976571e+20\n",
      "alpha = 10, eta = 100, mse = 1.264900349254894e+28\n",
      "alpha = 10, eta = 1000, mse = 9.558944620766703e+34\n",
      "alpha = 10, eta = 10000, mse = 1.5140463880930048e+42\n",
      "alpha = 10, eta = 100000, mse = 3.430932582276631e+48\n",
      "alpha = 10, eta = 1000000, mse = 9.659338174470282e+55\n",
      "alpha = 100, eta = 1e-06, mse = 589.8940694220533\n",
      "alpha = 100, eta = 1e-05, mse = 589.612439879341\n",
      "alpha = 100, eta = 0.0001, mse = 588.0339504572294\n",
      "alpha = 100, eta = 0.001, mse = 573.6743652634376\n",
      "alpha = 100, eta = 0.01, mse = 4761.95120928941\n",
      "alpha = 100, eta = 0.1, mse = 64612058097.81077\n",
      "alpha = 100, eta = 1, mse = 2.0983199527288123e+19\n",
      "alpha = 100, eta = 10, mse = 1.0100301113952757e+27\n",
      "alpha = 100, eta = 100, mse = 5.72374119127788e+33\n",
      "alpha = 100, eta = 1000, mse = 6.951055348268077e+40\n",
      "alpha = 100, eta = 10000, mse = 1.4669538629435048e+48\n",
      "alpha = 100, eta = 100000, mse = 8.883538827845762e+54\n",
      "alpha = 100, eta = 1000000, mse = 1.2878360944642058e+62\n",
      "alpha = 1000, eta = 1e-06, mse = 589.9090905446271\n",
      "alpha = 1000, eta = 1e-05, mse = 589.7147829101814\n",
      "alpha = 1000, eta = 0.0001, mse = 588.0658283832292\n",
      "alpha = 1000, eta = 0.001, mse = 544.5364544355944\n",
      "alpha = 1000, eta = 0.01, mse = 1031724048.2219979\n",
      "alpha = 1000, eta = 0.1, mse = 4.191064975824121e+17\n",
      "alpha = 1000, eta = 1, mse = 1.7776356905920756e+25\n",
      "alpha = 1000, eta = 10, mse = 6.24020791526353e+31\n",
      "alpha = 1000, eta = 100, mse = 9.88849568256572e+38\n",
      "alpha = 1000, eta = 1000, mse = 4.695174807431067e+45\n",
      "alpha = 1000, eta = 10000, mse = 1.6036288443600027e+53\n",
      "alpha = 1000, eta = 100000, mse = 8.68388373451371e+59\n",
      "alpha = 1000, eta = 1000000, mse = 9.307488858052187e+66\n",
      "alpha = 10000, eta = 1e-06, mse = 589.9199056028966\n",
      "alpha = 10000, eta = 1e-05, mse = 589.7869961491052\n",
      "alpha = 10000, eta = 0.0001, mse = 581.8554710248783\n",
      "alpha = 10000, eta = 0.001, mse = 7251217.401187607\n",
      "alpha = 10000, eta = 0.01, mse = 2755485801617960.0\n",
      "alpha = 10000, eta = 0.1, mse = 1.3690605034504221e+23\n",
      "alpha = 10000, eta = 1, mse = 2.4417512470202173e+30\n",
      "alpha = 10000, eta = 10, mse = 2.304993756801872e+37\n",
      "alpha = 10000, eta = 100, mse = 3.1685201350930324e+43\n",
      "alpha = 10000, eta = 1000, mse = 3.637437526366965e+51\n",
      "alpha = 10000, eta = 10000, mse = 9.875373102408463e+57\n",
      "alpha = 10000, eta = 100000, mse = 4.004453770828702e+64\n",
      "alpha = 10000, eta = 1000000, mse = 1.1642272586577377e+72\n",
      "alpha = 100000, eta = 1e-06, mse = 589.9194305857052\n",
      "alpha = 100000, eta = 1e-05, mse = 589.3099208259993\n",
      "alpha = 100000, eta = 0.0001, mse = 94453.31538506552\n",
      "alpha = 100000, eta = 0.001, mse = 8550020602803.298\n",
      "alpha = 100000, eta = 0.01, mse = 2.9272854113137234e+21\n",
      "alpha = 100000, eta = 0.1, mse = 3.307214456358955e+28\n",
      "alpha = 100000, eta = 1, mse = 1.1410798486385933e+35\n",
      "alpha = 100000, eta = 10, mse = 4.77887790606958e+41\n",
      "alpha = 100000, eta = 100, mse = 8.338581661462512e+48\n",
      "alpha = 100000, eta = 1000, mse = 2.1606889407584934e+55\n",
      "alpha = 100000, eta = 10000, mse = 8.37591693314114e+62\n",
      "alpha = 100000, eta = 100000, mse = 2.260886381978548e+69\n",
      "alpha = 100000, eta = 1000000, mse = 9.707849249469603e+76\n",
      "alpha = 1000000, eta = 1e-06, mse = 589.8205051447088\n",
      "alpha = 1000000, eta = 1e-05, mse = 854.5595158855186\n",
      "alpha = 1000000, eta = 0.0001, mse = 373309699443.9021\n",
      "alpha = 1000000, eta = 0.001, mse = 2.6147330189803405e+19\n",
      "alpha = 1000000, eta = 0.01, mse = 1.3783170131650721e+26\n",
      "alpha = 1000000, eta = 0.1, mse = 1.0349403514594418e+33\n",
      "alpha = 1000000, eta = 1, mse = 1.580316784034871e+40\n",
      "alpha = 1000000, eta = 10, mse = 1.2132135100163603e+47\n",
      "alpha = 1000000, eta = 100, mse = 6.138549912463654e+53\n",
      "alpha = 1000000, eta = 1000, mse = 1.4733007415825903e+61\n",
      "alpha = 1000000, eta = 10000, mse = 1.3417713764121098e+68\n",
      "alpha = 1000000, eta = 100000, mse = 1.4664982821346577e+75\n",
      "alpha = 1000000, eta = 1000000, mse = 4.5387344269484654e+81\n"
     ]
    }
   ],
   "source": [
    "#Here we are performing cross validation w.r.t our training dataset using custom built SGD Regressor. Here our hyperparameters-\n",
    "#- alpha and eta0.\n",
    "error_i = []\n",
    "err_1 = dict()\n",
    "alpha = [10**-6,10**-5,10**-4,10**-3,10**-2,10**-1,1,10**1,10**2,10**3,10**4,10**5,10**6]\n",
    "eta0 = [10**-6,10**-5,10**-4,10**-3,10**-2,10**-1,1,10**1,10**2,10**3,10**4,10**5,10**6]\n",
    "for i in alpha:\n",
    "    for j in eta0:\n",
    "        sgd = sgdreg_fit(d_train,alpha = i,eta0 = j)#Fitting our training dataset\n",
    "        y_pred = sgdreg_predict(sgd['weight'],sgd['intercept'],standardized_train) #predicting y_i's of train data using-\n",
    "        # the weights that our model has learned.\n",
    "        err = error(y_train,y_pred) #calculating our mean squared error\n",
    "        error_i.append(err) #Appending our errors\n",
    "        err_1[err] = [i,j,err] #storing the mean squared error corresponding to a particular alpha and eta0 value.\n",
    "        print('alpha = {0}, eta = {1}, mse = {2}'.format(i,j,err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.1, 37.84139064712439]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = min(error_i)#Finding the alpha and eta0 values corresponding to minimum mean squared error\n",
    "err_1[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are trying to predict the y_i's corresponding to x_i's of test dataset using our best fit model above\n",
    "sgd = sgdreg_fit(d_train,alpha = 0.1,eta0 = 0.1)\n",
    "y_pred = sgdreg_predict(sgd['weight'],sgd['intercept'],standardized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.59034191193969\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xt8VPWZ+PHPkxhLuJSIWqoRha6KCkgiwUtRN+AqurWIQkG8VLSt9tfftm5bseivXcXaFUtbe9vV6mqxixJYqLFeWrBClLpVIVxERIpVUCIKCokJBA3J8/vjnEkmk7mcuZyZzJzn/XrlRebMmTPf+TI5zznP9yaqijHGmOAqynUBjDHG5JYFAmOMCTgLBMYYE3AWCIwxJuAsEBhjTMBZIDDGmICzQGCySkSqRWSHD8edKSJ/yfRxC4GI3C4iC3w47nwRuTPTxzXZZ4EgYESkTkT2isinPO4/VERURA7xu2z5RETuE5HfRdk+WkQ+FpFBSRyrWkQ6RKRFRJpFZIuIXJvZEhsTmwWCABGRocA5gAKTclqY/PcwcJmI9IvYfjXwpKruSfJ476pqf+DTwPeAB0TklMidLCAbP1ggCJYvAy8C84Frwp8QkVIR+amIbBeRJhH5i4iUAs+7uzS6V6xnRaYaIu8aRORaEdnsXt2+KSI3eCmciNwrIj+J2Pa4iHzH/X22iPzdPe5rInJpjOP0uItx74S+Gvb4OreMe0VkmYgc524XEblHRHaJyEcislFERka+h6r+FWgApoQdsxi4Avid+/h0EVnjHud9EflZojpQRy2wFzgl7LN8RUTeBla4xz5TRP5XRBpFZIOIVIeVY5iIPOfW0zPAEbHez62Di8MeHyIiu0XkNPfx/4jIe+534nkRGRHjOD1Sc265j3d//5SI/ERE3nbr4j73+4WIHCEiT7qfZY+IrBIROzdlkVV2sHwZeMT9mSgig8Oe+wkwBvg8MAi4GegAznWfL1PV/u4JMJFdwMU4V7fXAveETiwJLASmi4gAiMhhwAVAjfv833HuaAYCc4AFInKUh+N2IyKXALcClwFHAqvc98Z9v3OBE933mQZ8GONQv8Op05B/AkqAp93HvwB+oaqfBv4BWOyhbEVugCsDNoY99Y/AyTj/b+XAU8CdOP9XNwFLReRId99HgXqcAPBDIoJ+hIXAjLDHE4EPVHWt+/iPwAnAZ4C1ON+dVMzFqdMK4HigHPg397nvAjtw/i8G4/zf2Nw3WWSBICBE5GzgOGCxqtbjnFSvcJ8rAq4DblTVBlVtV9X/VdWPU3kvVX1KVf/uXt0+ByzHOYEnsgrnBBDadyrwV1V91z3u/6jqu6raoaqLgK3A6SkU8evAXaq6WVUPAv8OVLh3BW3AAOAkQNx9dsY4zn8D/ygix7iPvww8qqpt7uM24HgROUJVW1T1xThlOlpEGoEPgNuAq1V1S9jzt6vqPlVtBa4CnlbVp926eAZYA/yziBwLjAV+oKofq+rzwBNx3vdRYJKI9HUfX0FXUERVH1LVZve7cDswWkQGxjleD25gvx74tqruUdVmnDq/3N2lDTgKOE5V21R1ldokaFllgSA4rgGWq+oH7uNH6bpSPALogxMc0iYiF4nIi+5tfiPwz8RJT4S4f/w1dF2hXkHYFaiIfFlE1rsphEZgpJfjRnEc8Iuw4+wBBChX1RXAr4H/AHaJyP0i8ukY5X0bJ3V2lYj0BybjpoVcX8G5Cn5dRFaHp2CieFdVy1R1kKpWqGpNxPPvRJT/S6Hyu5/hbJyT6dHAXlXdF7b/9lhvqqpvAJuBL7rBYBLOdwMRKRaRuW467iNgm/uyZOv8SKAvUB9W3j+52wHmAW8Ay91U4uwkj2/SZA1PAeDmYqcBxSLynrv5U0CZiIzGSUEcwElfbIh4ebQrs304f9ghnw17r08BS3Gujh9X1TYRqcU50XqxEOeEMBc4A7jUPe5xwAPAeTh3Ce0isj7GcUMnwb7AR5FlxDmp/khVo6Y5VPWXwC9F5DM46ZxZwA9ilPdhnMbdncBb7t1W6DhbgRnuHddlwBIROTziJO1V+P/DO8B/q+rXIndy6+kwEekX9j7HEj/VEkoPFQGvucEBnEB8CU7KaxtOqmwvseu88zshIuH1/QHQCoxQ1YYeH8y5Q/gu8F23PWaFiKxW1WfjlNlkkN0RBMNkoB04BSdHW4GTb14FfFlVO4CHgJ+JyNHuleBZ7kl9N05bwefCjrceOFdEjnXTBLeEPXcoTpDZDRwUkYtw8u6eqOo6nBPHfwHLVLXRfaofzslsNzgN0jh3BNGOsRunIfcq97NchxPkQu4Dbgk1fIrIQBH5kvv7WBE5Q0RKcE5uB9zPH8tSnBPtHJyg0ElErhKRI936DX2OeMfyagHOFfxE9/P1EacL6jGquh0nTTRHRA51U4JfTHC8Gpz/o/+DezfgGgB8jNNG0hcnnRPLBmCEiFSISB+cNBIA7ud/AKet6DMAIlIuIhPd3y8WkePdFFITznc1E/VkPLJAEAzXAL9V1bdV9b3QD04K5EpxetfchHNnsBonVXI3UKSq+4EfAS+4t/VnujnpRcArOI2ST4beyL26+xbOlfRenKvKPyRZ3kdxrkI7T0qq+hrwU+CvwPvAKOCFOMf4Gs6V/IfACOB/w471mPv5atyUx6vARe7Tn8Y5ae3FSal8iJO6iMq96l4KHEPPhtQLgU0i0oLTcHy5m+NPi6q+g3OlfitOYHwH57OG/p6vwLmb2oPT3tBjvEPE8Xbi1Ovncf5fQ36HUwcNwGs4Pc5iHeNvwB3An3HabiIH930PJ/3zolvnfwaGu8+d4D5uccvxn6q6Ml6ZTWaJtckYY0yw2R2BMcYEnAUCY4wJOAsExhgTcBYIjDEm4HwfRyDO/CtrgAZVvVhEhuF0Vzscp8fJ1ar6SbxjHHHEETp06FC/i+qrffv20a9f5PxkwWX10cXqojurjy7p1kV9ff0Hqnpkov2yMaDsRpyRi6HRmXcD96hqjYjchzP68t54Bxg6dChr1qzxt5Q+q6uro7q6OtfF6DWsPrpYXXRn9dEl3boQkZijysP5mhpy52D5As7goNCcIxOAJe4uD+MMdjLGGJMjvo4jEJElwF04IxRvAmYCL6pqaGraIcAfVbXHCFERuR5noioGDx48pqYmcuqV/NLS0kL//v1zXYxew+qji9VFd1YfXdKti/Hjx9eralWi/XxLDbkTbO1S1XoJmyvdK1W9H7gfoKqqSvP9VtFud7uz+uhiddGd1UeXbNWFn20E43Cmt/1nnJktP40zzL5MRA5xp/89Bmf4etLa2trYsWMHBw4cyFiB/TRw4EA2b96c62L0GlYfXfr3709bWxslJSW5LooJKN8CgaregjsZmXtHcJOqXiki/4Mzz3wNzhw4j6dy/B07djBgwACGDh2Ku45Jr9bc3MyAAQNyXYxew+rDoars2LGDHTt2MGzYsFwXxwRULsYRfA/4joi8gdOF9MFUDnLgwAEOP/zwvAgCxsQiIgwcODBv7mxNdtSua2Dc3BVsbGhi3NwV1K5LKXHiWVbWI1DVOqDO/f1NUltVqgcLAqYQ2PfYhKtd18CsJRtoa1cYAg2Nrcxa4iwTMrmy3Jf3tJHFxhjTi8x5YpMTBMK0tStzntjk23taIEhTbW0tIsLrr7+ecN/58+fz7rvvpvxedXV1XHxxvNUOs3ucaJ577jnOOuusbtsOHjzI4MGDPX32uro6Bg4cSEVFBSeffDJz5sxJqzy33347P/nJT9I6RiaPY0wie/e3JbU9EywQpGnhwoWcffbZLFy4MOG+6QaCfHDOOeewY8cOtm/vGtD45z//mREjRnD00Ud7Psb69etZs2YNCxYsYO3atd2eP3jwYEbLbEzQBSYQhBpfhs1+KmONLy0tLfzlL3/hwQcfJHLA2913382oUaMYPXo0s2fPpra2ljVr1nDllVdSUVFBa2srQ4cO5YMPnLXk16xZ09lf+OWXX+ass86isrKSz3/+82zZsiVuOc4880w2beq6bayurmbNmjWejhN5pTty5Ei2bdsGwIIFCzj99NOpqKjghhtuoL29nfb2dmbOnMnIkSMZNWoU99xzT7fjFRUVMW3atG71UVNTw4wZznr0v/zlLznllFM466yzuPzyy+N+rn79+jFmzBjeeOMN5s+fz6RJk5gwYQLnnXceAPPmzWPs2LGceuqp3HbbbZ2v+9GPfsSJJ57I2WefHfUzNzU1cdxxx9HR4ayGuG/fPoYMGUJbWxsPPPAAY8eOZfTo0UyZMoX9+/f3eH2ofgE++OADQvNgtbe3M2vWrM4y/eY3vwFg586dnHvuuVRUVDBy5EhWrVoV93ObYCsrjd6NONb2TAhEIKhd18Atv99IQ2MritP4csvvN6YdDB5//HEuvPBCTjzxRA4//HDq6511y//4xz/y+OOP89JLL7FhwwZuvvlmJk+eTFVVFY888gjr16+ntLQ05nFPOukkVq1axbp167jjjju49dZb45Zj+vTpLF68GHBOOjt37qSqqirp44TbvHkzixYt4oUXXmD9+vUUFxd3lr2hoYFXX32VjRs3cu211/Z47YwZMzoDwccff8zTTz/NlClTAJg7dy7r1q3jr3/9K/fdd1/cMnz44Ye8+OKLjBgxAoC1a9eyZMkSnnvuOZYvX87WrVt5+eWXWb9+PfX19Tz//PPU19dTU1PD+vXrefrpp1m9enWP44ZST8899xwATz75JBMnTqSkpITLLruM1atXs2HDBk4++WQefNB7p7YHH3yQgQMHsnr1alavXs0DDzzAW2+9xaOPPsrEiRNZv349GzZsoKKiwvMxTfDcPmkEJUXdOxCUFAm3Txrh23tmpddQrs1btoXWtvZu21rb2pm3bEtarfALFy7kxhtvBODyyy9n4cKFjBkzhj//+c9ce+219O3bF4BBgwbR3Nzs+bhNTU1cc801bN26FRGhrS1+bnDatGlccMEFzJkzh8WLFzN16tSUjhPu2Wefpb6+nrFjxwLQ2trKZz7zGb74xS/y5ptv8s1vfpMvfOELXHBBz3Xpq6qqaGlpYcuWLWzevJkzzjiDQYMGAXDqqady5ZVXMnHixM67hEirVq2isrKSoqIiZs+ezYgRI1i9ejXnn39+53GWL1/O8uXLqaysBJy7s61bt9Lc3Myll17aWfeTJk2K+h7Tp09n0aJFjB8/npqaGr7xjW8A8Oqrr/L973+fxsZGWlpamDhxouc6W758Oa+88gpLljhTaTU1NbF161bGjh3LddddR1tbG5MnT7ZAYOIKnZPmLdsCNFNeVsqsicN96zEEAQkE7zZGXy881nYv9uzZw4oVK9i4cSMiQnt7OyLCvHkx1znv4ZBDDulMT4T3I//BD37A+PHjeeyxx9i2bVvCIebl5eUcfvjhvPLKKyxatKjzStvLccLLEF4OVeWaa67hrrvu6vGaDRs2sGzZMu677z4WL17MQw891GOf0F3B5s2bu53wn3rqKZ5//nmWLl3Kz372MzZu3Mghh3T/Gp5zzjk8+eSTPY4ZPh2vqnLLLbdwww03dNvn5z//ebQq6mHSpEnceuut7Nmzh/r6eiZMmADAzJkzqa2tZfTo0cyfP5+6uroer431/6aq/OpXv4oaPJ5//nmeeuopZs6cyXe+8x2+/OUveyqnCabJleVMriynrq6Ob15Z7fv7BSI1dHRZ9DRMrO1eLFmyhKuvvprt27ezbds23nnnHYYNG8aqVas4//zz+e1vf9uZX96zZw8AAwYM6HZnMHTo0M500tKlSzu3NzU1UV7uRP/58+d7Ks/06dP58Y9/TFNTE6eeeqrn4wwdOrSzMXbt2rW89dZbAJx33nksWbKEXbt2dX6G7du388EHH9DR0cGUKVO48847ezTkhsyYMYMFCxawYsUKLrnkEgA6Ojp45513GD9+PHfccQdNTU20tLR4+nyRJk6cyEMPPdT5+oaGBnbt2sW5555LbW0tra2tNDc388QTT0R9ff/+/Rk7diw33ngjF198McXFxYAz4vmoo46ira2NRx55JGadhf7fQlf/oTLde++9nXdef/vb39i3bx/bt29n8ODBfO1rX+OrX/1qzDozJiTbA8oCEQhmTRxOaUlxt22lJcXMmjg85WMuXLiQSy+9tNu2KVOmsHDhQi688EImTZpEVVUVFRUVnY2xM2fO5Otf/3pnY/Ftt93GjTfeSFVVVeeJCODmm2/mlltuobKy0nMPmalTp1JTU8O0adOSOs6UKVPYs2cPI0aM4Ne//jUnnngiAKeccgp33nknF1xwAaeeeirnn38+O3fupKGhgerqaioqKrjqqqui3jEAnHzyyfTr148JEyZ0Xsm3t7dz1VVXMWrUKM4++2y+9a1vUVZW5unzRbrgggu44oorOOussxg1ahRTp06lubmZ0047jenTpzN69GguuuiiztRWNNOnT2fBggVMnz69c9sPf/hDzjjjDMaNG8dJJ50U9XU33XQT9957L5WVlZ2N/QBf/epXOeWUUzjttNMYOXIkN9xwAwcPHqSuro7Ro0dTWVnJokWLOtOJxkQT3qYJmWvTjMfXaagzpaqqSiMXptm8eTMnn3yy52PUrmtg3rItvNvYytFZyLlFsrl1urP66NLc3MyOHTuS+j4XsqDPPjpu7orOIPDdUQf56UYndVpeVsoLsyckdSwRye001L1NKOdmjDG9mR9tmokEIjVkjDH5wo82zUTyOhDkQ1rLmETse2zC+dGmmUjepob69OnDhx9+aFNRm7ymqjQ1NdGnT59cF8X0EjaOIAnHHHMMO3bsYPfu3bkuiicHDhywP/YwVh9d9u3bx+jRo3NdDNOLZHscQd4GgpKSkrxa0amurq5zFKyx+ghXV1dny1SanMrrNgJjjDHps0BgjDEBZ4HAGGMCzgKBMcYEnAUCY4wJOAsExhgTcBYIjDEm4CwQGGNMwFkgMMaYgLNAYIwxAWeBwBhjsiS0BOWw2U/FXYIy20tV5u1cQ8YYk09CS1C2trUDXUtQAt1mFu2235DY+2WS3REYY0wWzFu2pTMIhLS2tbvTTSe/XyZZIDDGmCzwugSlLVVpjDEFyusSlLZUpTEmbV4bJE12eV2C0paqNMakxWuDpMm+8CUo321s5egYS1DaUpXGmLTEa2i0QJB7oSUove6XraUqLTVkTAHJRUOjyX8WCIwpILloaDT5zwKBMQVk/ElHIhHb/G5oNPnPt0AgIn1E5GUR2SAim0Rkjrt9mIi8JCJviMgiETnUrzIYEyS16xpYWt+Ahm0TYMoYb3lpE1x+3hF8DExQ1dFABXChiJwJ3A3co6rHA3uBr/hYBmMCI1pDsQIrX9+dmwKZvOFbryFVVaDFfVji/igwAbjC3f4wcDtwr1/lMCYoYjUINyTRUFy7riFh90ZTeMQ5X/t0cJFioB44HvgPYB7wons3gIgMAf6oqiOjvPZ64HqAwYMHj6mpqfGtnNnQ0tJC//79c12MXsPqo0um6mLLe8180t4R9bkhg/pSVloS9/WNrW007G2lI+ycUCRC+WGlCV+bSfbd6JJuXYwfP75eVasS7efrOAJVbQcqRKQMeAw4KYnX3g/cD1BVVaXV1dW+lDFb6urqyPfPkElWH13SrYvQVXxDYxGxsr3lZcW8MDv+e4ybu4KGxuIe2728NpPsu9ElW3WRlQFlqtooIiuBs4AyETlEVQ8CxwA2/t30ar05XRI5kjgWL+MIbAxCcPnZa+hI904AESkFzgc2AyuBqe5u1wCP+1UGY9IVOtE2NLaidE3Z0Fvm74nWQByNl3EENgYhuPzsNXQUsFJEXgFWA8+o6pPA94DviMgbwOHAgz6WwZi05GJu+EjxJpHzcrXudRxBLiY7M72Dn72GXgEqo2x/Ezjdr/c1JpPSTZekm1ZKNInc0WWlUXsFFYvQoZrUe3qdFM0UHpt0zpg4Yp1oE6VLatc1MOeJTezd39a5LdZMoI2tbYybuyLqyTfRJHKzJg7v0UZQWlLMXZeNSukE7nVSNFNYbIoJY+JIJV0SuooPDwIhkWml2nUNNOxtjdkGkeiOZHJlOXddNoryslIEKC8rTTkImOCyOwJTUDLdwyeVdEmiBtzwk/u8ZVu4fEj3sTzhV/xe7kjsKt6kywKBKRh+LcqS7Ik2UftB+En83cZWGBL9GLXrGtj/ycEezwW5Abc3d+XNZ5YaMgUjVj79Xxetz+qSjfHaDyJP4rH2HVhaEjW9VFZaEtjUT2/vypvPLBCYghHvSjybJ41o7QrQ/SQe6hIaLe1TWlKMCFHTS42tbcxbtiWQJ7/e0JW3UFlqyBSMWPn0kFSWbEwlFZGoXSHaaGDBmZExtD7ttxetj3n8hsZWvr1oPf+6aL3v69n2plSMjXz2jwUCUzCidaWMlMxJI502h3jtCrGmiy4vK+WF2RM694kX1ELNy34uTu9Xm0uqUu3KaxKz1JDJa+Gjbuct28KUMeWUxzkxJHPS8CsVEW+66NBn2ffxQUqKI9cai86v9EhvS8XYyGf/WCAweSta4+HS+gZmTRzOz6dXpH3S8CsVESsYCXR+lsbWNlA4rK+36Z/9SI/0tlSMjZnwjwUCk7cSjbqNdtIAYs7bE8mvSdiiXdmG2gjCtXUofQ89JGpQi6SQ8Z5RvXESusmV5bwwewJvzf0CL8yeYEEgQywQmLzlZdRt+EkDSKr7oV+piMggdWhxUY8gEP5ZwvcHeixOH5LpnlGWigkOayw2eSvZxsNEdxCR/JyELbwxua6ujvKyjrifJXz/roVoeu6fSs+oeGUEm4QuCCwQmLwVa8K1WFesqeS8szV9QzKfJVSmYbOfinonkckcvk1fEQwJU0MiMk5E+rm/XyUiPxOR4/wvmjHxJdt4GOtOocxjg6yfUmkI7Y05fJOfvNwR3AuMFpHRwHeB/wJ+B/yjnwUzxotkrlhnTRzOrCUbaGvvfh3dcuAgtesacn7lm+zVd7S7iFDPo3FzV1gax3jmpbH4oKoqcAnwa1X9D2CAv8UyJvMmV5bT79Ce1z5tHZq1vvHxVhtLVrRG5MiBZkGcisIkz0sgaBaRW4CrgadEpAjI/b20MSloau25RgBkp298tHEPs5Zs4LWdH6UcGEI9o8rLSnu0F9g8PMYrL4FgOvAxcJ2qvgccA8zztVTG+CSXefVovZba2pX2Dk17Ns3eNvjL5JeEgcA9+S8FPuVu+gB4zM9CGROLl9RKvH1y2Tfey0k51at4azg26fDSa+hrwBLgN+6mcqDWz0IZE0201Mq3F63n+7Ub4+4TfpWdi2kKQoEp1qCxSKlcxdvgL5MOL72G/i9wOvASgKpuFZHP+FoqY6KINWvnIy++TdVxg5hcWe5p0FgyvXPSnYY52pTTiaRyFW+Dv0w6vASCj1X1ExFnYLuIHELPaVGM8V2sK2WFznRKrKmbU7nKzsQ0zPHWLz6sbwktB7ovRZnOVbwN/jKp8hIInhORW4FSETkf+AbwhL/FMkEX7Uo83sIz4SfpaFK5yvY6JUW8u4ZYAUiAdf92AbXrGnh/y1rELWNvvIrvTYvTGH94CQSzga8AG4EbgKdxBpUZ44tYV+JTxpTzyItvR70dLRaJeeWd6lW2l544ie4aEs2HNLmynLqmrbw1tzrp8mVDb1ucxvjDS6+hDlV9QFW/pKpT3d8tNWR8E+tKfOXru7nyzGN7zL5ZWlJMe5yv5JQxTttBsn31vfTESbR4S6JG3Np1DWx5rzkjA8z80NsWpzH+8NJr6C0ReTPyJxuFM8EU70r8zsmjuGd6RY9eP7FWJSsrLWFpfUOPQVwVc5YnPPl66YnjZSrsWL2UQlfbn7R3pD2OwC82PiEYvKSGqsJ+7wN8CRjkT3FM0CTTFhBtSuZw0WbvFCHqIK5Gd4RxvFSHl544XqbCjlXeZKfFzgVbJzgYvKSGPgz7aVDVnwNfyELZTIGL1ed//ElHRr0SH3/SkTEHisW68t67P/qUEuHipToSrYiVTv/9fLjatvEJwZDwjkBETgt7WIRzh2DrGJik9LjyH93OvBdjtwXcddmobvuPP+lIltY39Gi0XLN9Dytf3x3ziv27izfEbT8IidUbKZF0+u/nw9W2jU8IBi8n9J+G/X4Q2AZM86U0piBF63nSsLedhsbo6/CGlmcMP9mMm7siatAI70UULc3jJQiA0+soVan23w9NI+38WTl649W2jU8ofAkDgaqOz0ZBTOGKlgvvUKVYJOqJukiEYbOf6nb1GW8wWbjIHHt5nLEH4bwGjEwKlbG3jyMwhS9mIBCR78R7oar+LPPFMYUo1km8XZXSkuIeQSJ0Ug6/wo83mCze+0VbvCWaWL2O/NbbxxGYYIjXWDwgwY8xnsTKeYd3/RSip2dCV/jRGi29vF9kI3JZaQklxd3fpzemY4zJpph3BKo6J5sFMYUr2lV5kUhnGiSUChk2+6morw+1GazZvifmyOLI9wsXmeO2KROM6c5Lr6E+OFNMjMAZRwCAql7nY7lMAYnW86T8sPYeJ99EvWhWvr47YRDod2hxwpO6NX4a052XFcr+G/gsMBF4DmeFsmY/C2UKT2R//LLSnqudJuqz7qV/fUmxl6+0MSacl7+a41X1B8A+VX0YZzDZGYleJCJDRGSliLwmIptE5EZ3+yAReUZEtrr/HpbeRzCpyOQi6pmSaNEYL/3rY61JbIyJzcs4gtBfVqOIjATeA7wsTHMQ+K6qrhWRAUC9iDwDzASeVdW5IjIbZ3bT7yVfdJMqP2eUTDf/Hi9t46UHUG8ajGVMvvASCO53r9p/APwB6O/+Hpeq7gR2ur83i8hmnGUuLwGq3d0eBuqwQJBVfs1x41eACQ8uZX1L+NQhRTS2tiF0H0dgvX+MSY2XQPBbVW3HaR/4XCpvIiJDgUqc5S4Hu0ECnLuLwakc06TOrzluMh1gatc1MOeJTd3mC9q7v43SkmJ+Pr2i8z2t948x6ZFESwuIyNvAn4BFwIpk1yIQkf44QeRHqvp7EWlU1bKw5/eqao92AhG5HrgeYPDgwWNqamqSedtep6Wlhf79++e6GABsea+ZT9o7emw/tLiI4Z9NfYjIxoammM8dWlzE4IF9OhuJQ/XR2NrG+00H+KRPcAxoAAAW+0lEQVS9o9s+ja1t7NjbSqyvW7pl7U1603ejN7D66JJuXYwfP75eVasS7eclEPQFLgYuB8bgLFNZo6p/SXhwkRLgSWBZaCSyiGwBqlV1p4gcBdSpatz7+aqqKl2zZk2it+vV6urqqK6u9rSv3/3coy2oXlpS3K1hNhXj5q6IO/o3/D3q6upoHHhCzHL8v8c2su+T2G0BArw1tzAmwU3muxEEVh9d0q0LEfEUCLxMQ71fVRer6mVABfBpnCv8RAUQ4EFgc8R0FH8ArnF/vwZ4PNGxgiTW1MyZ7NWTqHdOvLLF62mUaPRv5HTPsVJJc57YFDcIQFejcG/s/WRMvvE0nbSI/CMwHbgQWIO32UfHAVcDG0VkvbvtVmAusFhEvgJs93iswMjWYiXJDqqK1xAMXbn6gaUl9CkpirkOQHg7RKw2iURrCIQahb02TttIYmPi8zKyeBuwDlgMzFLVfV4O7KaOYs3te57XAgaN34uVpHpSjHf1fqCto/O5xlanMfewviVRT+jh3TuTmUguRKDz7iXW1NThQdMWXzcmMS8Dyk5V1UtVdaHXIGBS52XB9FSlk3aKd/Ue7WSsSsKVrWKNJI426jjknukVnSdwL0HTFl83JjEvbQQfZaMgxuHn0oDpnBSTDURNrW1x2yEaW9s6yxOadTS0z+2TRvSoAwGuOvPYHusFJyprPiwHaUyu2ZKTvYyfSwOmc1KMNqq3tKS4c3BXpKPLSmO2Q9Sua6Bhb2vnCmWhdQkiP2eiOohVpvCgmQ/LQRqTaxYIeiG/ZsdM56QYK0Ct2b6HBS++3WP/8ScdGfNY85Zt4fIh3bstR+b2vdSBl6DpJVgYE3S2QlmApHtSjHZyjpVWWvn67pjHebexFYbE2J6kRAHDFl83+SjUqePyIc38v7krfP/OxrsjCA3bHA6Mxen/D/BF4GXfSmR848dJMV66KVYPJecOpOdM5n6la2z9AZNPuvV0G5Kdnm4JVygTkeeB01S12X18OxB9KSnT62X6pDjQnQ4i2vZY3TZnTRzOjs313fYvKZaodyY2BsAETbbGEoXz0n10MPBJ2ONPsIniDM5Juvnjg1Gfa2vviN9DKXJmkygznWRjlLUxvU0uerp5CQS/A14Wkdvdu4GXcKaPNgE3b9kW2juiz1UVa4qIdxtbmbdsCxpx5m/r0B7tDTYGwASRn2OJYvEyjuBHwLXAXvfnWlX9d99KZPJGKlcoR5eVer7isTEAJoj8HEsUi9cFXvsCH6nqL4AdIjLMtxKZvFHWN/YI4NKSophfZq9XPLm4MjIm18InhQTvk0KmI2EgEJHbcFYQu8XdVAIs8K1EJm/Em8H8YIcyZUx51JHFsyYOp0i6T0MV7YonF1dGxvQGkyvLeWH2BEaVD+SF2RN87yDhZUDZpTiri60FUNV33TWITcDFWyi+rV1Z+fpuXpg9ocdzkyvLqX3vNcrLiuP2BrIxAMZkh5dA8ImqqogogIj087lMJk8kmj00Xi6/rLSEF2ZXJ3wPGwNgjP+8BILFIvIboExEvgZcB/yXv8UyfojWJx9Sv+KONlI5nJdcfrrjBGycgTHpSxgIVPUnInI+8BHOKON/U9VnfC+Zyaho8/LP+p8NIE4aJ7QtmRGMoX1u/8OmHoPKvOTy010rwNYaMCYzvDQW362qz6jqLFW9SVWfEZG7s1E4kznR+uS3dWhnEAhJtp/+5Mpy1t92AT+fXhG1YTjeUpLpjhOwcQbGZIaX1ND5OL2Gwl0UZZvpxZLpe5+pyd/iXbGXxXkfr+9v4wyMyYyYdwQi8n9EZCNwkoi8EvbzFrAx1utM75RM3/tM9dNPdMWe7jgBG2dgTGbESw09ijPT6OPuv6GfMap6ZRbKZjIoWp/8aDLZTz/WlXlDYytb3mumobG1x6LWpSXFjD/pyJjppHA2zsCYzIg3+2gT0CQivwD2hM0++mkROUNVX8pWIU36QmmbOU9sirqoPECxSEZHMMbqXirAJ+0dQBHqPlactoXxJx3J0voGTw3ANs7AmMzw0kZwL3Ba2OOWKNtMlqXabfJAW0fU7aUlxRkfxh6te2nopB8uFARemD2BcXNXJDUFr40zMCZ9XgKBqHZNJqCqHSJiS1zmUKrdJqPl7CHzdwIh0a7YYw1AC6WRrAHYmOzzckJ/U0S+hXMXAPAN4E3/imQSSXXhilgn0w5Vz0Eg2TuRyCv2cXNXxF032RabNyb7vMw++nXg80ADsAM4A7jez0KZ+LxcNUfrv59uL5tMLBSTqIHXGoCNyT4vI4t3AZdnoSyBks7UCGV9S6I2+IZO6LFSR1PGlHdriIXuJ9lEZcrEEnqh/d7fshZxyxz+PtYAbEz2xQwEInKzqv5YRH5FlIUEVfVbvpasgKUzNULtugZaDvRcHjJ8zd9YJ+yVr+/mrstGRT3JeilTpvL3kyvLqWvayltzq2M+byd+Y7In3h3BZvffNdkoSJCkc2U9b9kW2qIsD1lSJMxbtoVvL1ofbflfwDlhxzrJeimT5e+NKUzxxhE84f5r6xNnWLJX1t+v3cjCl96hPc5KMPvbOtif4Mo83gk73uCvkGjdQS1/b0z+i5caeoIoKaEQVZ3kS4kCIJkr6+/XbmTBi2+n/Z6JTtjxBn/Vrmvodidh+XtjCku81NBP3H8vAz5L1/KUM4D3/SxUoUvmynrhS++k9V7RGmRjlSlaWkmhW3rI8vfGFJ6Y3UdV9TlVfQ4Yp6rTVfUJ9+cK4JzsFbHwhC9OHTltc6R46SDBWelLIifsCXPlmcd6WvN0cmV53LaFTAl1a93Y0BR3HiFjgizbfydeBpT1E5HPqeqbACIyDLDlKtPk9cq6WCRqMCgW4afTRnPL7zfGXUT+ETettPL13QnTOeU+NwZ365k0xBaSMSaaXPydeBlQ9m2gTkTqROQ5YCXwr76UxvQw44whMbfHmjIinOIEAy+DwPwezGULyRiTWC7+ThIGAlX9E3ACcCPwLWC4qi7zrUSmmzsnj+KqM4+l2M3/FItw1ZnHcufkUZ5TNpE3DK1t7cx5YlOP/ZJJWaXC5hEyJrFc/J0kTA2JSF/gO8Bxqvo1ETlBRIar6pO+lSoAkhlZfOfkUdw5eVSP7fEmcUtk7/42Ku9YTuP+tm7v72djcKIR0caY3IzX8ZIa+i3wCXCW+7gBuNO3EgVAJubsAW+LzcRpR2bv/ra03j8ZXkZEG2NyM9+Wl0DwD6r6Y6ANQFX3E//8YhLIVA4wWirnqjOPpay0pHOf0hIv/8X+5yBjjYjud+gh1lBsTJjwv2vIfIo2Gi+9hj4RkVLcVLOI/APwcaIXichDwMXALlUd6W4bBCwChgLbgGmqujelkuexTOYAI1M5tesaWFrfdWW/P8ZCNJl6/3SP3dQafbU0Y4Is9HddV1fHN6+s9v39vFwu3gb8CRgiIo8AzwI3e3jdfODCiG2zgWdV9QT3OLO9FzU50aZh7i3H9nPRdS89iWLxMwdpC80b03vFDQQiIsDrOKOLZwILgSpVrUt0YFV9HtgTsfkSIDR30cPA5OSK602mcvB+HdvPHGCqV/XJvH8qgdDWGTCm9xKNNxoJEJGNqtqzy4qXg4sMBZ4MSw01qmqZ+7sAe0OPo7z2etwFcAYPHjympqbG8/tuea/ZXRy9u0OLixj+2QFJforMHLulpYX+/ft3Pm5sbeP9pgN80t7BocVFDB7Yp1tuP9PliyeZ929sbaNhbysdYd+bIhHKDytN+Prwz3xUXzikT9+MfOZ8F/ndCDqrjy7p1sX48ePrVbUq0X5e2gjWishYVV2dcmmiUFUVkXiT2t0P3A9QVVWl1dXVno997eyn0Cg3OwIx58D3+9h1dXUk8xlS1biuIe5U1JFCi8Z75Sw12bOnUnlZMS/MrvZ8nGzVRz6wuujO6qNLturCSxvBGcCLIvJ3EXlFRDaKyCspvt/7InIUgPvvrhSPE5ef+ejenuueXFnOlWce26NbV0mxUFLUfWsqqRkbFGZM4fESCCYCnwMmAF/E6Qn0xRTf7w/ANe7v1wCPp3icuPzMR+dDrvvOyaO4Z3pFt26l86aOZt6XRqc9ari3B0JjTPLirUfQB2fh+uOBjcCDqtpzRFDs1y8EqoEjRGQHTu+jucBiEfkKsB2YlnrRY/Nz3vx8mZM/1gjhdMtpi9MYU3jitRE8jDOIbBVwEXAKznxDnqjqjBhPnee5dGnwc6qEIM/Jny+B0BjjXbxAcEqot5CIPAi8nJ0imVQkM3dRuoIcCI0pRPECQeeQT1U9KPFWPzE51W3+cmyef2NMcuI1Fo8WkY/cn2bg1NDvIvJRtgpoErN5/o0x6Yh5R6Cq8ae1NL2Gdek0xqTDy4CyQMlmrj1TcjF/uTGmcHibozgg/JyjyE+9bWyDnxP+GWMyzwJBmHzNtfu9xGQy8jWYGhNklhoKk8+59t7SpTNeMO0N5TPG9GR3BGFs+oT05XMwNSaoLBCESSbXbnnw6CyYGpN/LBCE8Zprtzx4bL2t4doYk5i1EUTwkmu3PHhsNheRMfnHAkEKLA8eX29puDbGeGOpoRRYHtwYU0gKNhD42ZhreXBjTCEpyNSQ37NxWh7cGFNICvKOIBsjhCdXljNr4nCOLivl3cZW5i3bYr2GjDF5qSDvCLLRmGtrABhjCkVB3hFkozE3X+clMsaYSAUZCLLRmGtdSI0xhaIgA0E2ZuO0LqTGmEJRkG0E4P+gplkTh3drIwDrQmqMyU8FGwj8ELl62ZQx5ax8fbd1ITXG5DULBB5F6yW0tL4hZwvAGGNMplgg8MgmmjPh8nFta2NisUDgUSZ6CdnJozDYGBJTaAqy15Af0u0l1NjaZmsYFAgbQ2IKjQUCj9Idm/B+0wE7eRQIG0NiCo0FAo/SHZvwSXtH1O128sg/NobEFBprI0hCOmMTDi2OHnPt5JF/bAyJKTQWCLJk8MA+lBS10dahndtKisROHnnIpiE3hcYCQTZJgscmb9hynKaQWBtBFtSua2DHnlba2rXb9rZ2tcZiY0zOWSDwWajPuaJRn7fGYmNMrlkg8Fm0PufhrLHYGJNr1kbgs3hX/NbTJH02WtuY9Nkdgc9iXfEXi9iEdWkKpd1stLYx6clJIBCRC0Vki4i8ISKzc1GGbIk1Ivmn00ZbEEiTTfVgTGZkPTUkIsXAfwDnAzuA1SLyB1V9LdtlyYbQyf79LWsRsPRFBtlUD8ZkRi7aCE4H3lDVNwFEpAa4BCjIQABOMKhr2spbc6tzXZSCcnRZKQ1RTvrWAG9MckQ1erdG395QZCpwoap+1X18NXCGqv5LxH7XA9cDDB48eExNTU1Wy5lpLS0t9O/fP9fF6DUyUR+NrW007G2lI+w7XCRC+WGllJWWpFvErLHvRndWH13SrYvx48fXq2pVov16ba8hVb0fuB+gqqpKq6urc1ugNNXV1ZHvnyGTMlUfhdBryL4b3Vl9dMlWXeQiEDQAQ8IeH+NuMyZpNtWDMenLRa+h1cAJIjJMRA4FLgf+kINyGGOMIQd3BKp6UET+BVgGFAMPqeqmbJfDGGOMIydtBKr6NPB0Lt470wohR22MCbZe21icD2wRc2NMIbApJtJgI1uNMYXAAkEabGSrMaYQWCBIgy1ibowpBBYI0hBrQjmbWtoYk0+ssTgNtoi5MaYQWCBIk41sNcbkO0sNGWNMwFkgMMaYgLNAYIwxAWeBwBhjAs4aiwuEzXlkjEmVBYICYHMeGWPSYamhAmBzHhlj0mGBoADYnEfGmHRYICgANueRMSYdFggKgM15ZIxJhzUWFwCb88gYkw4LBAXC5jwyxqTKUkPGGBNwFgiMMSbgLBAYY0zAWSAwxpiAs0BgjDEBJ6qa6zIkJCK7ge25LkeajgA+yHUhehGrjy5WF91ZfXRJty6OU9UjE+2UF4GgEIjIGlWtynU5egurjy5WF91ZfXTJVl1YasgYYwLOAoExxgScBYLsuT/XBehlrD66WF10Z/XRJSt1YW0ExhgTcHZHYIwxAWeBwBhjAs4CgQ9E5CER2SUir4ZtGyQiz4jIVvffw3JZxmwRkSEislJEXhORTSJyo7s9qPXRR0ReFpENbn3McbcPE5GXROQNEVkkIofmuqzZIiLFIrJORJ50Hwe5LraJyEYRWS8ia9xtvv+tWCDwx3zgwohts4FnVfUE4Fn3cRAcBL6rqqcAZwL/V0ROIbj18TEwQVVHAxXAhSJyJnA3cI+qHg/sBb6SwzJm243A5rDHQa4LgPGqWhE2fsD3vxULBD5Q1eeBPRGbLwEedn9/GJic1ULliKruVNW17u/NOH/w5QS3PlRVW9yHJe6PAhOAJe72wNSHiBwDfAH4L/exENC6iMP3vxULBNkzWFV3ur+/BwzOZWFyQUSGApXASwS4PtxUyHpgF/AM8HegUVUPurvswAmWQfBz4Gagw318OMGtC3AuCpaLSL2IXO9u8/1vxVYoywFVVREJVL9dEekPLAX+VVU/ci78HEGrD1VtBypEpAx4DDgpx0XKCRG5GNilqvUiUp3r8vQSZ6tqg4h8BnhGRF4Pf9KvvxW7I8ie90XkKAD33105Lk/WiEgJThB4RFV/724ObH2EqGojsBI4CygTkdCF2TFAQ84Klj3jgEkisg2owUkJ/YJg1gUAqtrg/rsL5yLhdLLwt2KBIHv+AFzj/n4N8HgOy5I1bs73QWCzqv4s7Kmg1seR7p0AIlIKnI/TbrISmOruFoj6UNVbVPUYVR0KXA6sUNUrCWBdAIhIPxEZEPoduAB4lSz8rdjIYh+IyEKgGmcK2feB24BaYDFwLM6U2tNUNbJBueCIyNnAKmAjXXngW3HaCYJYH6fiNPgV41yILVbVO0TkczhXxYOAdcBVqvpx7kqaXW5q6CZVvTiodeF+7sfch4cAj6rqj0TkcHz+W7FAYIwxAWepIWOMCTgLBMYYE3AWCIwxJuAsEBhjTMBZIDDGmICzQGDynohMFhEVkYQjdEVkpogcncZ7VYdmyUxHpo5jTCZYIDCFYAbwF/ffRGYCKQcCYwqRBQKT19w5jM7Gmar48ojnvufO7b5BROaKyFSgCnjEne+91J3//Qh3/yoRqXN/P11E/urOk/+/IjI8QTleFJERYY/r3OMlPI6I3C4iN4U9ftWdoA8Rucpdv2C9iPzGnbCuWETmu/ttFJFvp1Z7xjhs0jmT7y4B/qSqfxORD0VkjDuJ2UXuc2eo6n4RGaSqe0TkX3BGsIYW/Yh13NeBc1T1oIj8E/DvwJQ45VgETANuc+eDOUpV14jIp5M8TicRORmYDoxT1TYR+U/gSmATUK6qI939yrwcz5hYLBCYfDcDZ6IycKYlmAHUA/8E/FZV9wOkMCR/IPCwiJyAMzVwSYL9FwPLcaYTmUbXfPrJHifcecAYYLUbsEpxJhx7AviciPwKeMp9X2NSZoHA5C0RGYQzY+Uod2reYkBFZFYShzlIV4q0T9j2HwIrVfVSN01TF+8g7tTBH7pzCU0Hvp7EccLLEF4OAR5W1VsiXyAio4GJ7vtMA66LVz5j4rE2ApPPpgL/rarHqepQVR0CvAWcg7Pgy7Ui0hc6gwZAMzAg7BjbcK66oXvKZiBd0x/P9FieRTiLrAxU1VeSOM424DS3nKcBw9ztzwJT3bnpQ2vXHue2aRSp6lLg+6HXGpMqCwQmn82ga7bGkKXADFX9E870vWvc1cBCjbHzgftCjcXAHOAX4iwU3h52nB8Dd4nIOrzfOS/BabBenORxlgKDRGQT8C/A3wBU9TWcE/1yEXkFJ7gdhbNiV537uRYAPe4YjEmGzT5qjDEBZ3cExhgTcBYIjDEm4CwQGGNMwFkgMMaYgLNAYIwxAWeBwBhjAs4CgTHGBNz/Bxsm7jymhyoqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(error(y_test,y_pred))\n",
    "plot(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "<pre>From above plot we can see that the actual values and predicted values are linearly aligned with each other which indicates that our custom model is predicting our actual values with very less error and is thus somewhat perfectly performing Stochastic Gradient Descent Regression.</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38355789, -0.65461713, -0.54872229,  1.18067096, -0.61003668,\n",
       "         3.54662039, -0.34283599, -0.79518735, -1.05936229, -0.67868092,\n",
       "        -1.81454486,  0.96494767, -2.69118232]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_1 = sgd['weight'] #weights corresponding to our best fit model\n",
    "w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sklearn SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = make_scorer(mean_squared_error) #As mean_squared_error is not a scoring parameter in Grid Search CV therefor we are-\n",
    "#- using make_score to make mean_squared_error as one of the scoring parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 169 candidates, totalling 1690 fits\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1690 out of 1690 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "#Here we are performing gridsearch cross validation on our dataset for our sgd regressor\n",
    "sgd = SGDRegressor(max_iter = 1000)\n",
    "param_grid = {'alpha':[10**-6,10**-5,10**-4,10**-3,10**-2,10**-1,1,10**1,10**2,10**3,10**4,10**5,10**6],\n",
    "              'eta0':[10**-6,10**-5,10**-4,10**-3,10**-2,10**-1,1,10**1,10**2,10**3,10**4,10**5,10**6]\n",
    "             }\n",
    "gscv = GridSearchCV(sgd,param_grid,cv = 10,scoring = mse,verbose = 1)\n",
    "gscv.fit(standardized_train,y_train)\n",
    "print('*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.687995052417175\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_eta0</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.049928</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>20.687995</td>\n",
       "      <td>18.543505</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'eta0': 0.01, 'alpha': 0.01}</td>\n",
       "      <td>169</td>\n",
       "      <td>36.317707</td>\n",
       "      <td>16.836803</td>\n",
       "      <td>...</td>\n",
       "      <td>10.782786</td>\n",
       "      <td>19.542829</td>\n",
       "      <td>17.750613</td>\n",
       "      <td>18.83383</td>\n",
       "      <td>20.281476</td>\n",
       "      <td>18.631596</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>8.070575</td>\n",
       "      <td>0.862831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "56       0.049928         0.000303        20.687995         18.543505   \n",
       "\n",
       "   param_alpha param_eta0                         params  rank_test_score  \\\n",
       "56        0.01       0.01  {'eta0': 0.01, 'alpha': 0.01}              169   \n",
       "\n",
       "    split0_test_score  split0_train_score       ...         split7_test_score  \\\n",
       "56          36.317707           16.836803       ...                 10.782786   \n",
       "\n",
       "    split7_train_score  split8_test_score  split8_train_score  \\\n",
       "56           19.542829          17.750613            18.83383   \n",
       "\n",
       "    split9_test_score  split9_train_score  std_fit_time  std_score_time  \\\n",
       "56          20.281476           18.631596      0.001803         0.00006   \n",
       "\n",
       "    std_test_score  std_train_score  \n",
       "56        8.070575         0.862831  \n",
       "\n",
       "[1 rows x 32 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(gscv.cv_results_)#Converting our grid search cross validation results into dataframe\n",
    "print(df['mean_test_score'].min()) #Here we are trying to get the minimum mean squared error\n",
    "best_df = df[df['mean_test_score']==[df['mean_test_score'].min()]] #Here we are tring to get the entire datapoint corresponding-\n",
    "#- to the min mean_squared_error.\n",
    "best_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = best_df['param_alpha'].index #Getting index corresponding to best values of hyperparameters which result in min error\n",
    "index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters are: alpha = 0.01 and eta0 = 0.01\n"
     ]
    }
   ],
   "source": [
    "print('Best hyperparameters are: alpha = {0} and eta0 = {1}'.format(best_df['param_alpha'][index[0]],best_df['param_eta0'][index[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are testing our model that we learnt above on our test dataset\n",
    "sgd = SGDRegressor(alpha = 0.01,eta0 = 0.01,max_iter = 1000)\n",
    "sgd.fit(standardized_train,y_train)\n",
    "y_pred = sgd.predict(standardized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.651453623469465\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(error(y_test,y_pred))\n",
    "plot(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "<pre>The above graph that we have got using sklearns Stochastic Gradient Descent Regressor model is very much similar to our plot that we got using custom implementation.</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75595421,  0.78835386,  0.08226338,  0.62230719, -1.85857673,\n",
       "        3.6593889 , -0.44289959, -2.70376005,  1.70737974, -1.63573206,\n",
       "       -1.92452995,  0.82438522, -2.63974891])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_2 = sgd.coef_ #weights corresponding to our best fit model\n",
    "w_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.233214598939649"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w_1-w_2) #Calculating euclidean distance between the weight vectors we got using sklearn sgd regressor and our-\n",
    "#- own custom implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>From above we can see that the weight vectors that we got using Custom implementation and sklearns implementation are very close to each other in 13 dimensional space as the distance between them is only 4.23 units.</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regressor</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Distance</th>\n",
       "      <th>alpha</th>\n",
       "      <th>learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Custom SGDRegressor</td>\n",
       "      <td>58.59</td>\n",
       "      <td>[0.38355789, -0.65461713, -0.54872229, 1.18067...</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sklearn SGDRegressor</td>\n",
       "      <td>33.65</td>\n",
       "      <td>[-0.75595421, 0.78835386, 0.08226338, 0.622307...</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Regressor    MSE  \\\n",
       "0   Custom SGDRegressor  58.59   \n",
       "1  Sklearn SGDRegressor  33.65   \n",
       "\n",
       "                                              Weight  Distance  alpha  \\\n",
       "0  [0.38355789, -0.65461713, -0.54872229, 1.18067...      4.23   0.10   \n",
       "1  [-0.75595421, 0.78835386, 0.08226338, 0.622307...      4.23   0.01   \n",
       "\n",
       "   learning_rate  \n",
       "0           0.10  \n",
       "1           0.01  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = prettytable.PrettyTable()\n",
    "list_1 = [['Custom SGDRegressor',58.59,[ 0.38355789, -0.65461713, -0.54872229,  1.18067096, -0.61003668,\n",
    "         3.54662039, -0.34283599, -0.79518735, -1.05936229, -0.67868092,\n",
    "        -1.81454486,  0.96494767, -2.69118232],4.23,0.1,0.1],['Sklearn SGDRegressor',33.65,[-0.75595421,  0.78835386,  0.08226338,  0.62230719, -1.85857673,\n",
    "        3.6593889 , -0.44289959, -2.70376005,  1.70737974, -1.63573206,\n",
    "       -1.92452995,  0.82438522, -2.63974891],4.23,0.01,0.01]]\n",
    "names = ['Regressor', 'MSE', 'Weight', 'Distance','alpha','learning_rate']\n",
    "pd.DataFrame(list_1,columns = names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>From above table we can see that our weight vectors that we got using custom sgd regressor implementation and sklearns sgd regressor implementation are very close to each other as the distance between them is only 4.23 units which means that both the weights vectors are very similar to each other. This fact is also reflected in the mean squared error values which we got for both of these weight vectors. With Custom SGD Regressor we got best fit model for alpha = 0.1 and eta ( or learning_rate) = 0.1. Where as for Sklearns sgd we got best fit model for alpha = 0.01 and eta = 0.01. </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
